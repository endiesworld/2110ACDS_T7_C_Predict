{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7e849a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# Clasification Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "I {**2110ACDS_T7**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### EDSA-Climate Change Belief Analysis 2022\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Our clients would like to know people's perception on climate change, whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received, which will increasing their insights and informing future marketing strategies.\n",
    "\n",
    "SWAT_Team_7 has been consulted to create a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "### Process\n",
    "\n",
    "- 1. analyse the supplied data;\n",
    "- 2. identify potential errors in the data and clean the existing data set;\n",
    "- 3. determine if additional features can be added to enrich the data set;\n",
    "- 4. build a model that is capable of forecasting the three hourly demand shortfalls;\n",
    "- 5. evaluate the accuracy of the best machine learning model;\n",
    "- 6. determine if a person believes in climate change or not, and\n",
    "- 7. explain the inner working of the model to a non-technical audience.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05600c92",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997462e2",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475dbe93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T10:30:53.800892Z",
     "start_time": "2021-06-23T10:30:50.215449Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Setting global constants to ensure notebook results are reproducible\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246051de",
   "metadata": {},
   "source": [
    "### Comet_ml Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c916166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment with your api key\n",
    "# Every individual in this team is expected to edit the Experiment parameters with their respective comet_ml values\n",
    "# experiment = Experiment(\n",
    "#     api_key=\"emBEBYBp72gW5tfeZBSGftD0Y\",\n",
    "#     project_name=\"tweet-sentiment-analyzer\",\n",
    "#     workspace=\"emmanuelokoro\",\n",
    "#     log_code = True\n",
    "# )\n",
    "\n",
    "# experiment = Experiment(\n",
    "#     api_key=\"pRMxFxeNwUPYOyNGu3BPn91GY\",\n",
    "#     project_name=\"advanced-classification\",\n",
    "#     workspace=\"jakam\",\n",
    "#     log_code = True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a6718",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbb6c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:35.311495Z",
     "start_time": "2021-06-28T08:49:35.295494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81132ab3",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c044ea",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "For this section, we carry out two different types of data analysis:\n",
    "- Univariate \\\n",
    "    i. non-graphical \\\n",
    "    ii. graphical \n",
    "- Multivariate \\\n",
    "    i. non-graphical \\\n",
    "    ii. graphical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e773ee3",
   "metadata": {},
   "source": [
    "#### Univariate Non-Graphical Analysis\n",
    "For this analysis, we are going to view dataset on the below checks:  \\\n",
    "    i.  Check for the presence of *null* values  \\\n",
    "    ii. Descriptive statistical values *mean, std, minimum, quatiles, maximum, and kurtosis*  \n",
    "    iii. Dataset data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e805134e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:52:37.824204Z",
     "start_time": "2021-06-28T08:52:37.811206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "message      0\n",
       "tweetid      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at data statistics\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254e20c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15819 entries, 0 to 15818\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  15819 non-null  int64 \n",
      " 1   message    15819 non-null  object\n",
      " 2   tweetid    15819 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 370.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check data types for all columns\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f855bfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <td>15819.0</td>\n",
       "      <td>0.917504</td>\n",
       "      <td>0.836537</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetid</th>\n",
       "      <td>15819.0</td>\n",
       "      <td>501719.433656</td>\n",
       "      <td>289045.983132</td>\n",
       "      <td>6.0</td>\n",
       "      <td>253207.5</td>\n",
       "      <td>502291.0</td>\n",
       "      <td>753769.0</td>\n",
       "      <td>999888.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count           mean            std  min       25%       50%  \\\n",
       "sentiment  15819.0       0.917504       0.836537 -1.0       1.0       1.0   \n",
       "tweetid    15819.0  501719.433656  289045.983132  6.0  253207.5  502291.0   \n",
       "\n",
       "                75%       max  \n",
       "sentiment       1.0       2.0  \n",
       "tweetid    753769.0  999888.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at data statistics\n",
    "df_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b05422a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0.122976\n",
       "tweetid     -1.193356\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf02c5b",
   "metadata": {},
   "source": [
    "From the above analysis thus far, it is evidence that we only have two numeric colunms. \n",
    "However we suspect that one of these columns(tweetid) contains unique values in each row, while the other column(sentiment) from the name, we infere that it is our label, hence contains a minimum of two different values.\n",
    "\n",
    "To confirm the above, we write a function that takes in a dataframe and a column-id, to give an output which is the number of unique values in the column as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e95c1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_val(df, col):\n",
    "    \"\"\"\n",
    "        This function takes in a dataframe and a column name, \n",
    "        and ouputs an interger, which is the number of unique \n",
    "        values in the column.\n",
    "    \"\"\"\n",
    "    return df[col].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a22e6980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numbers of unique values in the sentiment column is : 4\n"
     ]
    }
   ],
   "source": [
    "# Check the numbers of unique values for the sentiment column\n",
    "print(f'The numbers of unique values in the sentiment column is : {unique_val(df_train, \"sentiment\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e48c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numbers of unique values in the tweetid column is : 15819\n"
     ]
    }
   ],
   "source": [
    "# Check the numbers of unique values for the tweetid column\n",
    "print(f'The numbers of unique values in the tweetid column is : {unique_val(df_train, \"tweetid\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e2880",
   "metadata": {},
   "source": [
    "From the above results, the sentiment column contains four different unique values, and we want to see how this values\n",
    "are distributed in the column.\n",
    "To achieve this, we write a function called *unique_val_count*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80cd6a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_val_count(df, col):\n",
    "    \"\"\"\n",
    "        This function takes in a dataframe and a column name, \n",
    "        and ouputs a dictionary, which contains the unique values as a key, and the numbers as values.\n",
    "    \"\"\"\n",
    "    distribution = {}\n",
    "    unique_vals = df[col].unique()\n",
    "    for val in unique_vals:\n",
    "        distribution[val] = df[df[col] == val][col].count()\n",
    "    \n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ddf06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 8530, 2: 3640, 0: 2353, -1: 1296}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_val_count(df_train, 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8fec5",
   "metadata": {},
   "source": [
    "### Check for presence of non-alphanumeric words\n",
    "\n",
    "This task will be done in the data engineering section, after we have removed all stop-words and punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd562db5",
   "metadata": {},
   "source": [
    "### Univariate graphical inspection of data\n",
    "For this analysis, we view the individual colunms using histogram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3f47bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar plot of label classes\n",
    "fig,ax = plt.subplots()\n",
    "df_train['sentiment'].value_counts().plot(kind = 'bar', facecolor='g', alpha=0.65)\n",
    "ax.set_xlabel('Sentiments')\n",
    "ax.set_ylabel('Sentiments count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74b341",
   "metadata": {},
   "source": [
    "# PUT IN WORD CLOUD BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1525812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77b34f68",
   "metadata": {},
   "source": [
    "## Also include findings from world cloud in EDA summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f2a7d",
   "metadata": {},
   "source": [
    "### EDA summary\n",
    "- The dataset contains three columns (sentiments, message and tweetid)\n",
    "- Sentiments and tweetid are of numeric data type, while message is non-numeric\n",
    "- tweetid is a clumn with uniques values acreoss the entire rows of the dataset\n",
    "- sentiments columns contains for different unique values (-1, 0, 1 &2) with different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de51df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at feature distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa93ec6",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ced808",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "\n",
    "### Removing Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba0d8c",
   "metadata": {},
   "source": [
    "For the given dataset, we identified that the *message* column contains the novel tweet for each userid, which we are espected to classify. \n",
    "For us to proceed we have to carry out cleaning on this messages. This cleaning will be achieved through:\n",
    "* identify and remove web-urls from the main message \n",
    "* idendify and remove words started with '#'\n",
    "* idendify and remove words started with '@'\n",
    "* making everything lower case\n",
    "* removing punctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab1ea8",
   "metadata": {},
   "source": [
    "#### Remove web-url from message\n",
    "\n",
    "We write a function called *delete_url*. This function uses regex to identify web-url in a column and remove same from the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "059c2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_url(data, col):\n",
    "    \"\"\"\n",
    "        Accepts a dataframe and col., removes web urls from the col.\n",
    "        returns a new dataframe \n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "    subs_url = ''\n",
    "    df[col] = df[col].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98fa8207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with message colun void of url links\n",
    "new_df_train = delete_url(df_train, 'message')\n",
    "new_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84eea17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PolySciMajor EPA chief doesn't think carbon dioxide is main cause of global warming and.. wait, what!?  via @mashable\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that the above operation was successful \n",
    "new_df_train['message'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59692724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PolySciMajor EPA chief doesn't think carbon dioxide is main cause of global warming and.. wait, what!? https://t.co/yeLvcEFXkC via @mashable\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['message'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b3c36",
   "metadata": {},
   "source": [
    "### Remove '#' and '@' words\n",
    "\n",
    "We write a function *delete_tags*, to identify and remove words started with '#' and '@'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92a0f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_tags(data, col):\n",
    "    \"\"\"\n",
    "        This function takes in a dataframe and a col, removes all words started with '#' and '@' in the column,\n",
    "        and returns a new dataframe\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    pattern_tags = r'#\\w+[#?]'\n",
    "    pattern_2 = r'@\\w+'\n",
    "    subs_tag = ''\n",
    "    df[col] = df[col].replace(to_replace = pattern_tags, value = subs_tag, regex = True)\n",
    "    df[col] = df[col].replace(to_replace = pattern_2, value = subs_tag, regex = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8422fbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT : Researchers say we have three years to ac...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>WIRED : 2016 was a pivotal year in the war on...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT : It's 2016, and a racist, sexist, climate ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT : Researchers say we have three years to ac...   698562\n",
       "3          1   WIRED : 2016 was a pivotal year in the war on...   573736\n",
       "4          1  RT : It's 2016, and a racist, sexist, climate ...   466954"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with message colun void of url links\n",
    "new_df_train = delete_tags(new_df_train, 'message')\n",
    "new_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ec814",
   "metadata": {},
   "source": [
    "### Convert capitalized words to lowercase words\n",
    "\n",
    "We write a function *word_converter* to convert capitalized words to lowercase words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcb93891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_converter(data, col):\n",
    "    \"\"\"\n",
    "        This function takes in a dataframe and col, converts all capitalized words in the column to lowercase,\n",
    "        and returns a new dataframe.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    df[col] = df[col].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ac78c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>it's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt : researchers say we have three years to ac...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wired : 2016 was a pivotal year in the war on...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt : it's 2016, and a racist, sexist, climate ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  polyscimajor epa chief doesn't think carbon di...   625221\n",
       "1          1  it's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  rt : researchers say we have three years to ac...   698562\n",
       "3          1   wired : 2016 was a pivotal year in the war on...   573736\n",
       "4          1  rt : it's 2016, and a racist, sexist, climate ...   466954"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with all words in the message column converted to its lowercase form\n",
    "new_df_train = word_converter(new_df_train, 'message')\n",
    "new_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f51a4",
   "metadata": {},
   "source": [
    "### Remove punctuation\n",
    "\n",
    "We write a function *remove_punc* that uses the string package from python to remove punctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87a4a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(data, col):\n",
    "    \"\"\"\n",
    "        This function takes in a dataframe and a column, uses python string package to identify and remove all\n",
    "        punctions in the column. It returns a new dataframe\n",
    "    \"\"\"\n",
    "    def operation(post):\n",
    "        return ''.join([l for l in post if l not in string.punctuation])\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    df[col] = df[col].apply(operation)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0deb07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt  researchers say we have three years to act...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wired  2016 was a pivotal year in the war on ...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt  its 2016 and a racist sexist climate chang...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   625221\n",
       "1          1  its not like we lack evidence of anthropogenic...   126103\n",
       "2          2  rt  researchers say we have three years to act...   698562\n",
       "3          1   wired  2016 was a pivotal year in the war on ...   573736\n",
       "4          1  rt  its 2016 and a racist sexist climate chang...   466954"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with the message colmn void of punctuations\n",
    "new_df_train = remove_punc(new_df_train, 'message')\n",
    "new_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb8adf",
   "metadata": {},
   "source": [
    "### Remove new lines (\\n)  and 'rt' from the start of any words\n",
    "\n",
    "We noticed that some words start with '\\n' and this is a short form for new line in programming, words started with \\n looses its original meaning.\n",
    "\n",
    "Hence we write a function remove_new_line to execute this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69ee9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_new_line(data, col):\n",
    "    \"\"\"\n",
    "        Takes in a dataframe and a column, returns a new dataframe with a new column void of new line command\n",
    "    \"\"\"\n",
    "\n",
    "    def operation(text):\n",
    "        result = re.sub(\"\\n\", \"\", text)\n",
    "        result = re.sub(\"rt\", \"\", result)\n",
    "        return result\n",
    "\n",
    "    df = data.copy()\n",
    "    \n",
    "    df[col] = df[col].apply(operation)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7548016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>researchers say we have three years to act o...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wired  2016 was a pivotal year in the war on ...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>its 2016 and a racist sexist climate change ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   625221\n",
       "1          1  its not like we lack evidence of anthropogenic...   126103\n",
       "2          2    researchers say we have three years to act o...   698562\n",
       "3          1   wired  2016 was a pivotal year in the war on ...   573736\n",
       "4          1    its 2016 and a racist sexist climate change ...   466954"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with the message colmn void of punctuations\n",
    "new_df_train = remove_new_line(new_df_train, 'message')\n",
    "new_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa36e01",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "We write a function *tokenizer* to tokenize the words in the message column and store same in a new column named *message_tok*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "897e0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(data, col):\n",
    "    \"\"\"\n",
    "        This function takes in a dataframe and a col, creates a new column to store the tokenized words\n",
    "        in the inputed column, and returns a new dataframe.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    tokeniser = TreebankWordTokenizer()\n",
    "    df['message_tok'] = df[col].apply(tokeniser.tokenize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea5182f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>message_tok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[its, not, like, we, lack, evidence, of, anthr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>researchers say we have three years to act o...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[researchers, say, we, have, three, years, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wired  2016 was a pivotal year in the war on ...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[wired, 2016, was, a, pivotal, year, in, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>its 2016 and a racist sexist climate change ...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[its, 2016, and, a, racist, sexist, climate, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   625221   \n",
       "1          1  its not like we lack evidence of anthropogenic...   126103   \n",
       "2          2    researchers say we have three years to act o...   698562   \n",
       "3          1   wired  2016 was a pivotal year in the war on ...   573736   \n",
       "4          1    its 2016 and a racist sexist climate change ...   466954   \n",
       "\n",
       "                                         message_tok  \n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...  \n",
       "1  [its, not, like, we, lack, evidence, of, anthr...  \n",
       "2  [researchers, say, we, have, three, years, to,...  \n",
       "3  [wired, 2016, was, a, pivotal, year, in, the, ...  \n",
       "4  [its, 2016, and, a, racist, sexist, climate, c...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column to hold the tokens from message column\n",
    "new_df_train = tokenizer(new_df_train, 'message')\n",
    "new_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93c845c",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "We write a function *stem_words* to transform all words in the *message_tok* column to its root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4957f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(data, col):\n",
    "    \"\"\"\n",
    "        Takes in a dataframe and a column, converts the words in the column to it root form,\n",
    "        with the aid of SnowballStemmer class from the nltk package.\n",
    "        Returns a new dataframe with an additional column \"message_stem\"\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    def operation(words, stemmer):\n",
    "        return [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    df = data.copy()\n",
    "    df[\"message_stem\"] = df[col].apply(operation, args=(stemmer, ))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09d2c7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>message_tok</th>\n",
       "      <th>message_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[its, not, like, we, lack, evidence, of, anthr...</td>\n",
       "      <td>[it, not, like, we, lack, evid, of, anthropoge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>researchers say we have three years to act o...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[researchers, say, we, have, three, years, to,...</td>\n",
       "      <td>[research, say, we, have, three, year, to, act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wired  2016 was a pivotal year in the war on ...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[wired, 2016, was, a, pivotal, year, in, the, ...</td>\n",
       "      <td>[wire, 2016, was, a, pivot, year, in, the, war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>its 2016 and a racist sexist climate change ...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[its, 2016, and, a, racist, sexist, climate, c...</td>\n",
       "      <td>[it, 2016, and, a, racist, sexist, climat, cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   625221   \n",
       "1          1  its not like we lack evidence of anthropogenic...   126103   \n",
       "2          2    researchers say we have three years to act o...   698562   \n",
       "3          1   wired  2016 was a pivotal year in the war on ...   573736   \n",
       "4          1    its 2016 and a racist sexist climate change ...   466954   \n",
       "\n",
       "                                         message_tok  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [its, not, like, we, lack, evidence, of, anthr...   \n",
       "2  [researchers, say, we, have, three, years, to,...   \n",
       "3  [wired, 2016, was, a, pivotal, year, in, the, ...   \n",
       "4  [its, 2016, and, a, racist, sexist, climate, c...   \n",
       "\n",
       "                                        message_stem  \n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...  \n",
       "1  [it, not, like, we, lack, evid, of, anthropoge...  \n",
       "2  [research, say, we, have, three, year, to, act...  \n",
       "3  [wire, 2016, was, a, pivot, year, in, the, war...  \n",
       "4  [it, 2016, and, a, racist, sexist, climat, cha...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column to hold root words from stemmer\n",
    "new_df_train = stem_words(new_df_train, 'message_tok')\n",
    "new_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c55afc",
   "metadata": {},
   "source": [
    "### Larmming\n",
    "\n",
    "We write a function *lam_words* to transform all words in the *message_tok* column to its root form using Lemmatization, to enable us acrter for the shortfall in stemming above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "148e0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lam_words(data, col):\n",
    "    \"\"\"\n",
    "        Takes in a dataframe and a column, converts the words in the column to it root form,\n",
    "        with the aid of WordNetLemmatizer class from the nltk package.\n",
    "        Returns a new dataframe with an additional column \"message_lam\"\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def operation(words, lemmatizer):\n",
    "        return [lemmatizer.lemmatize(word) for word in words] \n",
    "    df = data.copy()\n",
    "    df[\"message_lam\"] = df[col].apply(operation, args=(lemmatizer, ))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea6ae293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>message_tok</th>\n",
       "      <th>message_stem</th>\n",
       "      <th>message_lam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[its, not, like, we, lack, evidence, of, anthr...</td>\n",
       "      <td>[it, not, like, we, lack, evid, of, anthropoge...</td>\n",
       "      <td>[it, not, like, we, lack, evidence, of, anthro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>researchers say we have three years to act o...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[researchers, say, we, have, three, years, to,...</td>\n",
       "      <td>[research, say, we, have, three, year, to, act...</td>\n",
       "      <td>[researcher, say, we, have, three, year, to, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wired  2016 was a pivotal year in the war on ...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[wired, 2016, was, a, pivotal, year, in, the, ...</td>\n",
       "      <td>[wire, 2016, was, a, pivot, year, in, the, war...</td>\n",
       "      <td>[wired, 2016, wa, a, pivotal, year, in, the, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>its 2016 and a racist sexist climate change ...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[its, 2016, and, a, racist, sexist, climate, c...</td>\n",
       "      <td>[it, 2016, and, a, racist, sexist, climat, cha...</td>\n",
       "      <td>[it, 2016, and, a, racist, sexist, climate, ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   625221   \n",
       "1          1  its not like we lack evidence of anthropogenic...   126103   \n",
       "2          2    researchers say we have three years to act o...   698562   \n",
       "3          1   wired  2016 was a pivotal year in the war on ...   573736   \n",
       "4          1    its 2016 and a racist sexist climate change ...   466954   \n",
       "\n",
       "                                         message_tok  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [its, not, like, we, lack, evidence, of, anthr...   \n",
       "2  [researchers, say, we, have, three, years, to,...   \n",
       "3  [wired, 2016, was, a, pivotal, year, in, the, ...   \n",
       "4  [its, 2016, and, a, racist, sexist, climate, c...   \n",
       "\n",
       "                                        message_stem  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [it, not, like, we, lack, evid, of, anthropoge...   \n",
       "2  [research, say, we, have, three, year, to, act...   \n",
       "3  [wire, 2016, was, a, pivot, year, in, the, war...   \n",
       "4  [it, 2016, and, a, racist, sexist, climat, cha...   \n",
       "\n",
       "                                         message_lam  \n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...  \n",
       "1  [it, not, like, we, lack, evidence, of, anthro...  \n",
       "2  [researcher, say, we, have, three, year, to, a...  \n",
       "3  [wired, 2016, wa, a, pivotal, year, in, the, w...  \n",
       "4  [it, 2016, and, a, racist, sexist, climate, ch...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column to hold root words from stemmer\n",
    "new_df_train = lam_words(new_df_train, 'message_tok')\n",
    "new_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4510e4f",
   "metadata": {},
   "source": [
    "### Remove stop words\n",
    "Stop words are words which do not contain important significance to be used in Search Queries. \n",
    "We write a function *remove_stop_word*, that removes stop words in a speified column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3ebade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data, col):\n",
    "    \"\"\"\n",
    "        Takes a dataframe and a column, creates a new dataframe with a new column no_stop_word from the input\n",
    "        dataframe and column, returns the new column\n",
    "    \"\"\"\n",
    "    def operation(toks):\n",
    "        new_toks = [tok for tok in toks if tok not in stopwords.words('english')]\n",
    "        return new_toks\n",
    "    \n",
    "    df = data.copy()\n",
    "    df['no_stop_word'] = df[col].apply(operation)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column from message_lam void of stop words\n",
    "new_df_train = remove_stop_words(new_df_train, 'message_lam')\n",
    "new_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0135cec",
   "metadata": {},
   "source": [
    "### Check for the presence of noise as non-alphanumeric worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45206759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_type_checker(data, col):\n",
    "    alphanum_count = 0 \n",
    "    non_alphanum_count = 0\n",
    "    \n",
    "    def operation(str_list , alphanum_count ,non_alphanum_count):\n",
    "        alphanum = alphanum_count\n",
    "        non_aphanum = non_alphanum_count\n",
    "#         print(f'past row values: {(alphanum, non_aphanum)}') # for testing \n",
    "        for strg in str_list:\n",
    "            if strg.isalnum():\n",
    "                alphanum = alphanum + 1\n",
    "            else:\n",
    "                non_aphanum = non_aphanum + 1\n",
    "        \n",
    "        return (alphanum ,non_aphanum)\n",
    "    \n",
    "    for label, sr in data.iterrows():\n",
    "        (alphanum_count, non_alphanum_count) = operation(sr[col] , alphanum_count, non_alphanum_count)\n",
    "#         Uncomment the codes below for testing of this function\n",
    "#         print(f'accumulated values: {(alphanum_count, non_alphanum_count)}') \n",
    "#         if label == 20:\n",
    "#             break\n",
    "    \n",
    "    return (alphanum_count, non_alphanum_count)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "(alphanumeric, non_alphanumeric) = word_type_checker(new_df_train, 'no_stop_word')\n",
    "(alphanumeric, non_alphanumeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef376fd",
   "metadata": {},
   "source": [
    "### Transforming text into numbers\n",
    "\n",
    "- CounterVectorizer\n",
    "- TfidfVectorizer\n",
    "\n",
    "Most models do not work well with text, hence the need to convert our text into numbers. To execute this task and more, we can use **CountVectorizer** or **TfidfVectorizer** packages from sklearn library.\n",
    "\n",
    "#### CountVectorizer\n",
    "**CountVectorizer** has some **hyperparameters** which we can asign desired values to while initialising. \n",
    "The **hyperparameters** that we shall be tunning for these work are: \n",
    "\n",
    "- **max_df** :  When building the vocabulary ignore terms that have a document frequency strictly higher than the                     given threshold (corpus-specific stop words). If float, the parameter represents a proportion of                       documents,integer absolute counts.This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "- **min_df** :  When building the vocabulary ignore terms that have a document frequency strictly lower than the given                 threshold. This value is also called cut-off in the literature. If float, the parameter represents a                   proportion of documents, integer absolute counts.This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "- **max_features**: If not None, build a vocabulary that only consider the top max_features ordered by term frequency                     across the corpus. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "- **analyzer**: Whether the feature should be made of word n-gram or character n-grams. Option ‘char_wb’ creates                       character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with                 space.\n",
    "\n",
    "- **ngram_range**: The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to                    be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_df = CountVectorizer(max_features=20000,analyzer='word', ngram_range=(1, 2))\n",
    "vect_downsampled = CountVectorizer(max_features=20000,analyzer='word', ngram_range=(1, 2))\n",
    "vect_upsampled = CountVectorizer(max_features=20000,analyzer='word', ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce372a9",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3521e22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f78213bc",
   "metadata": {},
   "source": [
    "### Convert processed words to corpus\n",
    "\n",
    "Before we can transform the words in numeric type, for each column we have to remove the delimeters introduced during tokennization. This process is needed to enable us form a **corpus**.\n",
    "To achieve this, we write a function **form_corpus**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c773f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_corpus(data, col):\n",
    "    \"\"\"\n",
    "        This function takes in a dataframe and a column with tokenized text, \n",
    "        returns a new dataframe with an additional column(de_tok), which is made up of all words in the inserted colunm\n",
    "        but void of delimeters.\n",
    "    \"\"\"\n",
    "    def operation(tok_list):\n",
    "        string = ' '.join(tok_list)\n",
    "        return string\n",
    "    df = data.copy()\n",
    "    df['de_tok'] = df[col].apply(operation)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38167fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column from no_stop_word void of delimeters\n",
    "new_df_train = form_corpus(new_df_train, 'no_stop_word')\n",
    "new_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b92c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop every other columns except sentiment and de_tok columns\n",
    "df_train_reduced = new_df_train[['sentiment', 'de_tok']]\n",
    "df_train_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfbc020",
   "metadata": {},
   "source": [
    "### Resampling\n",
    "- **Downsampling the majority class**\n",
    "- **Upsampling the minority class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554a327",
   "metadata": {},
   "source": [
    "To achieve the above, we write a multipurpose function called **resampler**, which we can use for both **Downsampling** and **Upsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampler(df_list, n_sample, replace=False):\n",
    "    \"\"\"\n",
    "        This function takes a list of dataframe, the size which we intend to resize all dataframes in the list to, and\n",
    "        an optional replace[bool] value, which is set to True for upsampling\n",
    "        It returns a tuple of new dataframes with sizes equivalent to the value for n_sample\n",
    "    \"\"\"\n",
    "    \n",
    "    def operation(df):\n",
    "        downsampled_df = resample(df,\n",
    "                          replace= replace, # sample without replacement (no need to duplicate observations)\n",
    "                          n_samples=n_sample, # match number in minority class\n",
    "                          random_state= RANDOM_STATE) # reproducible results\n",
    "        return downsampled_df\n",
    "    \n",
    "    resampled_dfs = tuple(map(operation , df_list))\n",
    "    \n",
    "    return resampled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6127d",
   "metadata": {},
   "source": [
    "#### Create new dataframes\n",
    "    We create new dataframes basedd on sentiment type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = df_train_reduced[df_train_reduced['sentiment'] == 1]\n",
    "anti = df_train_reduced[df_train_reduced['sentiment'] == -1]\n",
    "neutral = df_train_reduced[df_train_reduced['sentiment'] == 0]\n",
    "info =  df_train_reduced[df_train_reduced['sentiment'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8489646",
   "metadata": {},
   "source": [
    "### Downsampling\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b09c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample pro, infor and neutral dataframe \n",
    "(pro_df, info_df, neut_df) = resampler([pro,info,neutral], len(anti))\n",
    "\n",
    "# Combine downsampled majority class with minority class\n",
    "downsampled = pd.concat([pro_df, info_df, neut_df,anti])\n",
    "\n",
    "# Check new class counts\n",
    "downsampled['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the downsampled dataframe\n",
    "original_height = [len(pro), len(neutral), len(info), len(anti)]\n",
    "downsampled_heights = [len(downsampled[downsampled['sentiment']==1]),len(downsampled[downsampled['sentiment']==0]),\n",
    "                      len(downsampled[downsampled['sentiment']==2]) ,len(downsampled[downsampled['sentiment']==-1])]\n",
    "\n",
    "# Get all possible labels\n",
    "labels = downsampled['sentiment'].unique()\n",
    "plt.bar(labels,original_height,color='grey')\n",
    "plt.bar(labels,downsampled_heights,color='orange')\n",
    "plt.xticks(labels,[1,0, 2, -1])\n",
    "plt.ylabel(\"# of observations\")\n",
    "plt.xlabel(\"Sentiments\")\n",
    "plt.legend(['original','resampled'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17527320",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8aeef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample info, neutral and anti dataframe \n",
    "(up_info_df, up_neut_df, up_anti_df) = resampler([info,neutral, anti], len(pro), True)\n",
    "\n",
    "# Combine downsampled majority class with minority class\n",
    "upsampled = pd.concat([pro, up_info_df, up_neut_df,up_anti_df])\n",
    "\n",
    "# Check new class counts\n",
    "upsampled['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the upnsampled dataframe\n",
    "original_height = [len(pro), len(neutral), len(info), len(anti)]\n",
    "upsampled_heights = [len(upsampled[upsampled['sentiment']==1]),len(upsampled[upsampled['sentiment']==0]),\n",
    "                      len(upsampled[upsampled['sentiment']==2]) ,len(upsampled[upsampled['sentiment']==-1])]\n",
    "\n",
    "# Get all possible labels\n",
    "labels = upsampled['sentiment'].unique()\n",
    "plt.bar(labels,upsampled_heights,color='orange')\n",
    "plt.bar(labels,original_height,color='grey')\n",
    "plt.xticks(labels,[1,0, 2, -1])\n",
    "plt.ylabel(\"# of observations\")\n",
    "plt.xlabel(\"Sentiments\")\n",
    "plt.legend(['original','resampled'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa2012",
   "metadata": {},
   "source": [
    "#### Data Transformation \n",
    "\n",
    "We use the instance of **CountVectorizer** **(vect_20)** created above to fit and transform all samples of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5dca8b",
   "metadata": {},
   "source": [
    "#### Data transformation for original data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584fc9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the de_tok column\n",
    "X_count = vect_df.fit_transform(df_train_reduced['de_tok'].values.astype(str))\n",
    "X_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb377f",
   "metadata": {},
   "source": [
    "#### Data transformation for Downsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count_downsampled = vect_downsampled.fit_transform(downsampled['de_tok'].values.astype(str))\n",
    "X_count_downsampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b875f0",
   "metadata": {},
   "source": [
    "#### Data transformation for Upsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb906a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count_upsampled = vect_upsampled.fit_transform(upsampled['de_tok'].values.astype(str))\n",
    "X_count_upsampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ed3ae",
   "metadata": {},
   "source": [
    "#### Separate features and dependant for all three dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951dbbe",
   "metadata": {},
   "source": [
    "#### Features and dependant for original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec762ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the dependant variable into a variable\n",
    "y = df_train_reduced['sentiment']\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade51fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See feature names\n",
    "# vect_df.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84351e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ece89",
   "metadata": {},
   "source": [
    "#### Features and dataset for downsampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db392edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the dependant downsampled dataset\n",
    "y_downsampled = downsampled['sentiment']\n",
    "y_downsampled[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bbffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count_downsampled.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See feature names\n",
    "# vect_downsampled.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d21349",
   "metadata": {},
   "source": [
    "#### Features and dataset for upsampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea78547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the dependant downsampled dataset\n",
    "y_upsampled = upsampled['sentiment']\n",
    "y_upsampled[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f47bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count_upsampled.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See feature names\n",
    "# vect_upsampled.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52905e3",
   "metadata": {},
   "source": [
    "### Convert all processed dataset back to dataframe form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117be10",
   "metadata": {},
   "source": [
    "### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe66104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(data=X_count.toarray(),columns = vect_df.get_feature_names())\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9f3c0",
   "metadata": {},
   "source": [
    "### Downsampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df = pd.DataFrame(data=X_count_downsampled.toarray(),columns = vect_downsampled.get_feature_names())\n",
    "downsampled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ece103",
   "metadata": {},
   "source": [
    "### upsampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6088e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_df = pd.DataFrame(data=X_count_upsampled.toarray(),columns = vect_upsampled.get_feature_names())\n",
    "upsampled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2d523",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to create one or more regression models that are able to accurately predict the thee hour load shortfall. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f8ca1",
   "metadata": {},
   "source": [
    "### Logistics Regression\n",
    "\n",
    "We proceed to create a **Logistics Regression model** for the three types of dataset we have:\n",
    "- original dataset\n",
    "- downsampled dataset\n",
    "- upsampled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b9364",
   "metadata": {},
   "source": [
    "#### Instanciate Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d073e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one or more ML models\n",
    "# logreg = LogisticRegression(multi_class='ovr')\n",
    "# logreg_down = LogisticRegression(multi_class='ovr')\n",
    "# logreg_up = LogisticRegression(multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dbc036",
   "metadata": {},
   "source": [
    "#### RandomForest Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logreg = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "logreg_down = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "logreg_up = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076f3bb",
   "metadata": {},
   "source": [
    "### Train the model with the original dataset\n",
    "\n",
    "We fit the model with the features(**model_df**) and the dependant (**y**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed192263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model generated above\n",
    "log_fit = logreg.fit(model_df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736bbf9",
   "metadata": {},
   "source": [
    "### Train the model with the downsampled dataset\n",
    "\n",
    "We fit the model with the features(**downsampled_df**) and the dependant (**y_downsampled**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b6f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model generated above\n",
    "downsampled_fit = logreg_down.fit(downsampled_df, y_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c732437a",
   "metadata": {},
   "source": [
    "### Train the model with the upsampled dataset\n",
    "\n",
    "We fit the model with the features(**upsampled_df**) and the dependant (**y_upsampled**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b245f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model generated above\n",
    "upsampled_fit = logreg_up.fit(upsampled_df, y_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b530251",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "To enable us make prediction with our model, we have to import the **test** dataset and execute all data engineering operation executed on the **train** dataset.\n",
    "\n",
    "These activities are: \n",
    "- **delete urls**\n",
    "- **delete tags**\n",
    "- **convert words to lowercases**\n",
    "- **remove punctions**\n",
    "- **remove newlines**\n",
    "- **tokenize**\n",
    "- **stemming**\n",
    "- **larmming**\n",
    "- **remove stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af6c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and view the first 5 rolls of our training dataset\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with message colun void of url links\n",
    "new_df_test = delete_url(df_test, 'message')\n",
    "new_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with message colun void of url links\n",
    "new_df_test = delete_tags(new_df_test, 'message')\n",
    "new_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf021a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with all words in the message column converted to its lowercase form\n",
    "new_df_test = word_converter(new_df_test, 'message')\n",
    "new_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f100a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with the message colmn void of punctuations\n",
    "new_df_test = remove_punc(new_df_test, 'message')\n",
    "new_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb8f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with the message colmn void of punctuations\n",
    "new_df_test = remove_new_line(new_df_test, 'message')\n",
    "new_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column to hold the tokens from message column\n",
    "new_df_test = tokenizer(new_df_test, 'message')\n",
    "new_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column to hold root words from stemmer\n",
    "new_df_test = stem_words(new_df_test, 'message_tok')\n",
    "new_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column to hold root words from stemmer\n",
    "new_df_test = lam_words(new_df_test, 'message_tok')\n",
    "new_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f489630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column from message_lam void of stop words\n",
    "new_df_test = remove_stop_words(new_df_test, 'message_lam')\n",
    "new_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column from no_stop_word void of delimeters\n",
    "new_df_test = form_corpus(new_df_test, 'no_stop_word')\n",
    "new_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ba967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop every other columns except sentiment and de_tok columns\n",
    "df_test_reduced = new_df_test[[ 'de_tok']]\n",
    "df_test_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0187fac2",
   "metadata": {},
   "source": [
    "### Transform test dataset \n",
    "\n",
    "We transform the test dataset using the three different obects of the text transfromer generated above:\n",
    "- **vect_df** : for original dataset\n",
    "- **vect_downsampled** : for downsampled dataset\n",
    "- **vect_upsampled** : for upsampled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4fd80",
   "metadata": {},
   "source": [
    "#### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the de_tok column\n",
    "X_count_test = vect_df.transform(df_test_reduced['de_tok'].values.astype(str))\n",
    "X_count_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348aaee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(data=X_count_test.toarray(),columns = vect_df.get_feature_names())\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01faf93",
   "metadata": {},
   "source": [
    "#### Downsampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d433248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the de_tok column\n",
    "X_count_test_downsampled = vect_downsampled.transform(df_test_reduced['de_tok'].values.astype(str))\n",
    "X_count_test_downsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6591fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_downsampled = pd.DataFrame(data=X_count_test_downsampled.toarray(),columns = vect_downsampled.get_feature_names())\n",
    "text_downsampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377b6a7",
   "metadata": {},
   "source": [
    "#### Upsampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27910064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the de_tok column\n",
    "X_count_test_upsampled = vect_upsampled.transform(df_test_reduced['de_tok'].values.astype(str))\n",
    "X_count_test_upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa31631",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_upsampled = pd.DataFrame(data=X_count_test_upsampled.toarray(),columns = vect_upsampled.get_feature_names())\n",
    "text_upsampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d59b1",
   "metadata": {},
   "source": [
    "### Make predictions with the models generated above and listed below:\n",
    "- **logreg** (original dataset)\n",
    "- **downsampled_fit** (downsampled dataset)\n",
    "- **upsampled_fit** (upsampled dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c6521",
   "metadata": {},
   "source": [
    "#### logreg (original dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction with model from original dataset\n",
    "y_pred_test = logreg.predict(text_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778b61a",
   "metadata": {},
   "source": [
    "## Personally, I feel we need to confirm the need for code below\n",
    "\n",
    "We need to confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7580ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and view the first 5 rolls of our training dataset\n",
    "# df_sample = pd.read_csv('data/sample_submission.csv')\n",
    "# y_exact = df_sample['sentiment']\n",
    "# df_sample.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2d5df",
   "metadata": {},
   "source": [
    "#### downsampled_fit (downsampled dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction with model from downsampled dataset\n",
    "y_pred_downsampled = downsampled_fit.predict(text_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef429d82",
   "metadata": {},
   "source": [
    "#### upsampled_fit (upsampled dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e23e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction with model from upsampled dataset\n",
    "y_pred_upsampled = upsampled_fit.predict(text_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1242af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b7e1a",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Classification report\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_exact, y_pred_test, labels= [1,2,0,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88e17a",
   "metadata": {},
   "source": [
    "### Convert the predictions to dataframe\n",
    "\n",
    "We have generated predictions three different predictions using three different models as stated below :\n",
    "- **model** : logreg , **prediction** : y_pred_test\n",
    "- **model** : downsampled_fit , **prediction** : y_pred_downsampled\n",
    "- **model** : upsampled_fit , **prediction** : y_pred_upsampled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0539b",
   "metadata": {},
   "source": [
    "#### model : logreg , prediction : y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best model and motivate why it is the best choice\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'tweetid': df_test['tweetid'],\n",
    "    'sentiment': y_pred_test\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('file_001.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d61e9",
   "metadata": {},
   "source": [
    "#### model : downsampled_fit , prediction : y_pred_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce4d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df = pd.DataFrame({\n",
    "    'tweetid': df_test['tweetid'],\n",
    "    'sentiment': y_pred_downsampled\n",
    "})\n",
    "\n",
    "downsampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df.to_csv('file_002.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96fbd9",
   "metadata": {},
   "source": [
    "#### model : upsampled_fit , prediction : y_pred_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e9357",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_df = pd.DataFrame({\n",
    "    'tweetid': df_test['tweetid'],\n",
    "    'sentiment': y_pred_upsampled\n",
    "})\n",
    "\n",
    "upsampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_df.to_csv('file_003.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a4499",
   "metadata": {},
   "source": [
    "### Logging Results to Comet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13998a",
   "metadata": {},
   "source": [
    "#### Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all metrics for model performance\n",
    "# f1 = f1_score(y_true, y_pred_test, average = 'weighted')\n",
    "# precision = precision_score(y_true, y_pred_test, average ='weighted')\n",
    "# recall = recall_score(y_true, y_pred_test,average = 'weighted')\n",
    "# accuracy = accuracy_score(y_true, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96baa1ad",
   "metadata": {},
   "source": [
    "### Create disctionary for data to be stored on Comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates two dictionaries, parameters and metric values\n",
    "# params = {\n",
    "#     'random_state': RANDOM_STATE,\n",
    "#     'model_type': 'Logistics Regression'\n",
    "# }\n",
    "\n",
    "# metrics ={\n",
    "#     'Accuracy': accuracy,\n",
    "#     'precision': precision,\n",
    "#     'recall': recall,\n",
    "#     'f1': f1,\n",
    "# }\n",
    "# metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793abcfc",
   "metadata": {},
   "source": [
    "### Log parameters and metrics to commet_ml\n",
    "\n",
    "In order to adher to a fundamental pricinple of programming **DRY**(do not repeat yourself), we define a function **experiment_logger** to achieve this. This is because this is a group project with multiple comet_ml API keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_logger(experiments):\n",
    "    \"\"\"\n",
    "        This function takes in a list of comet_ml defined experiments, and logs the parameters and metric values,\n",
    "        for this notebook to the respective experiments. It has a return value of None\n",
    "    \"\"\"\n",
    "    for experiment in experiments :\n",
    "        experiment.log_parameters(params)\n",
    "        experiment.log_metrics(metrics)\n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cbd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a list of comet experiments for this project\n",
    "# comet_experiments = [ experiment]\n",
    "\n",
    "# # Log the respective experiments parameter and metric values\n",
    "# experiment_logger(comet_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da3fb7d",
   "metadata": {},
   "source": [
    "### End all comet experiment after loging the parameters\n",
    "\n",
    "To adhre to the **DRY** principle, we write a function **end_comet** to achieve this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_comet(experiments):\n",
    "    \"\"\"\n",
    "        This function takes in a list of comet_ml defined experiments, and ends the experiments.\n",
    "        It has a return value of None.\n",
    "    \"\"\"\n",
    "    for experiment in experiments :\n",
    "        experiment.end()\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9335189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # End all experiments\n",
    "# end_comet(comet_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad0c0d",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd1f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
